name: 'MCP Tool Evaluator'
description: 'Run MCP tool evaluations'
author: 'Matthew Lenhard'
branding:
  icon: 'check-circle'
  color: 'green'

inputs:
  evals_path:
    description: 'Path to the evals file'
    required: true
    default: 'evals.ts'
  openai_api_key:
    description: 'OpenAI API key'
    required: true
  model:
    description: 'Model to use for evaluation'
    required: false
    default: 'gpt-4'
  timeout:
    description: 'Timeout in milliseconds for each tool call'
    required: false
    default: '5000'
  server_path:
    description: 'Path to the MCP server code (relative to repository root)'
    required: false
    default: '.'

outputs:
  accuracy:
    description: 'Accuracy score from 0-100'
  completeness:
    description: 'Completeness score from 0-100'
  relevance:
    description: 'Relevance score from 0-100'
  clarity:
    description: 'Clarity score from 0-100'
  reasoning:
    description: 'Reasoning score from 0-100'
  overall_comments:
    description: 'Detailed comments about the evaluation'

runs:
  using: 'composite'
  steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-node@v4
      with:
        node-version: '20'
    - run: npm install mcp-evals
      shell: bash
    - id: run-evals
      run: |
        # Run evaluations and output the results
        npx mcp-evals ${{ inputs.evals_path }} ${{ inputs.server_path }} > eval_output.txt
        
        # Extract all JSON objects from output
        grep -Pzo '(?s)\{[^{]*"accuracy"[^}]*\}' eval_output.txt > eval_results.json
        
        echo "results<<EOF" >> $GITHUB_OUTPUT
        cat eval_results.json >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT
      shell: bash
      env:
        OPENAI_API_KEY: ${{ inputs.openai_api_key }}
        MODEL: ${{ inputs.model }}
        TIMEOUT: ${{ inputs.timeout }}
    - uses: actions/github-script@v7
      with:
        script: |
          const resultsText = `${{ steps.run-evals.outputs.results }}`;
          // Split by newlines to get individual JSON objects
          const resultsArray = resultsText.split('\n').filter(line => line.trim());
          
          let body = '## MCP Tool Evaluation Results\n\n';
          
          // Process each result
          for (let i = 0; i < resultsArray.length; i++) {
            try {
              const results = JSON.parse(resultsArray[i]);
              
              // Get eval name if available, or use generic name
              const evalName = results.name || `Evaluation ${i+1}`;
              body += `### ${evalName}\n\n`;
              
              // Create table headers
              let table = '| Metric | Score | Comments |\n| ------ | ----- | -------- |\n';
              
              // Add scores to table
              table += `| Accuracy | ${results.accuracy} | ${results.accuracy_comments || '-'} |\n`;
              table += `| Completeness | ${results.completeness} | ${results.completeness_comments || '-'} |\n`;
              table += `| Relevance | ${results.relevance} | ${results.relevance_comments || '-'} |\n`;
              table += `| Clarity | ${results.clarity} | ${results.clarity_comments || '-'} |\n`;
              table += `| Reasoning | ${results.reasoning} | ${results.reasoning_comments || '-'} |\n`;
              
              body += table;
              
              // Add overall comments if they exist
              if (results.overall_comments) {
                body += `\n#### Overall Comments\n\n${results.overall_comments}\n\n`;
              }
              
            } catch (error) {
              console.error(`Error parsing result ${i}: ${error.message}`);
              body += `\n### Error parsing evaluation ${i+1}\n\n`;
            }
          }
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          }); 